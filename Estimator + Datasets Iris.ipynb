{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Datasets and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and Estimators are two key TensorFlow features:\n",
    "\n",
    "- Datasets: Best practice way of creating input pipelines. Reading data in to your graph. \n",
    "- Estimators: A high-level API to create TensorFlow models. Estimators include canned models (out of the box) and custom estimators.\n",
    "\n",
    "Below you the TensorFlow architecture including the dataset API an Estimators. Combined, they offer an easy way to create TensorFlow models:\n",
    "\n",
    "![title](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model categorizes Iris flowers based on four botanical features (sepal length, sepal width, petal length, and petal width). So, during inference, you can provide values for those four features and the model will predict that the flower is one of the following three beautiful variants:\n",
    "\n",
    "![title](https://www.tensorflow.org/images/iris_three_species.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erwinh/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import six.moves.urllib.request as request\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/tmp/tf_dataset_and_estimator_apis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = request.urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of the features\n",
    "feature_names = [\n",
    "    'SepalLength', \n",
    "    'Sepal_Width', \n",
    "    'PetalLength', \n",
    "    'PetalWidth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we'll need a function that reads the input file and returns the feature and label data. Estimators requires that you create a function of the following format. The return value must be a two-element tuple organized as follows: :\n",
    "\n",
    "The first element must be a dict in which each input feature is a key, and then a list of values for the training batch.\n",
    "The second element is a list of labels for the training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        # Convert CSV records to tensors. Each column maps to one tensor.\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element (label)\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "    \n",
    "    #A Dataset comprising lines from one or more text files\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    \n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note the following: :\n",
    "- TextLineDataset: The Dataset API will do a lot of memory management for you when you're using its file-based datasets. You can, for example, read in dataset files much larger than memory or read in multiple files by specifying a list as argument.\n",
    "- shuffle: Reads buffer_size records, then shuffles (randomizes) their order.\n",
    "- map: Calls the decode_csv function with each element in the dataset as an argument (since we are using TextLineDataset, each element will be a line of CSV text). Then we apply decode_csv to each of the lines.\n",
    "- decode_csv: Splits each line into fields, providing the default values if necessary. Then returns a dict with the field keys and field values. The map function updates each elem (line) in the dataset with the dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'SepalLength': array([5. , 5.1, 5.1, 5. , 5.2, 7. , 5.6, 6.7, 5.7, 5.8, 4.9, 5.1, 4.8,\n",
      "       6.1, 5.6, 7.2, 6.6, 7.6, 6. , 5.2, 5.9, 5.2, 7.9, 7.4, 6.4, 5. ,\n",
      "       5.1, 5. , 4.9, 5.7, 6.5, 5. ], dtype=float32), 'Sepal_Width': array([3.5, 3.8, 2.5, 3.5, 3.4, 3.2, 2.9, 3. , 2.8, 4. , 2.5, 3.8, 3.1,\n",
      "       2.6, 2.7, 3.6, 3. , 3. , 2.9, 3.5, 3.2, 2.7, 3.8, 2.8, 3.2, 3.3,\n",
      "       3.8, 2. , 2.4, 3. , 3. , 3.4], dtype=float32), 'PetalWidth': array([0.3, 0.2, 1.1, 0.6, 0.2, 1.4, 1.3, 2.3, 1.3, 0.2, 1.7, 0.3, 0.2,\n",
      "       1.4, 1.3, 2.5, 1.4, 2.1, 1.5, 0.2, 1.8, 1.4, 2. , 1.9, 2.3, 0.2,\n",
      "       0.4, 1. , 1. , 1.2, 1.8, 0.4], dtype=float32), 'PetalLength': array([1.3, 1.6, 3. , 1.6, 1.4, 4.7, 3.6, 5.2, 4.5, 1.2, 4.5, 1.5, 1.6,\n",
      "       5.6, 4.2, 6.1, 4.4, 6.6, 4.5, 1.5, 4.8, 3.9, 6.4, 6.1, 5.3, 1.4,\n",
      "       1.9, 3.5, 3.3, 4.2, 5.5, 1.6], dtype=float32)}, array([0, 0, 1, 0, 0, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 1, 1,\n",
      "       2, 2, 2, 0, 0, 1, 1, 1, 2, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "next_batch = input_fn(FILE_TRAIN, True) # Will return 32 random elements\n",
    "\n",
    "# Now let's try it out, retrieving and printing one batch of data.\n",
    "# Although this code looks strange, you don't need to understand\n",
    "# the details.\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and specify your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this case we are building a three layer network. Here we specify the amount of hidden units\n",
    "num_hidden_units =[512, 256, 128] \n",
    "number_classes = 3\n",
    "directory = './Checkpoints/checkpoints_tutorial17-1/\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all estimators make use of input_fn that provides the estimator with input data. In our case, we will reuse input_fn, which we defined for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1037b6b50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './Checkpoints/checkpoints_tutorial17-1/\"', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=num_hidden_units,\n",
    "    n_classes = 3, \n",
    "    model_dir=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./Checkpoints/checkpoints_tutorial17-1/\"/model.ckpt.\n",
      "INFO:tensorflow:loss = 34.649323, step = 1\n",
      "INFO:tensorflow:global_step/sec: 283.743\n",
      "INFO:tensorflow:loss = 7.8025436, step = 101 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.448\n",
      "INFO:tensorflow:loss = 0.5647123, step = 201 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.741\n",
      "INFO:tensorflow:loss = 0.28406665, step = 301 (0.280 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 375 into ./Checkpoints/checkpoints_tutorial17-1/\"/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.24058434.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x11a461c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(FILE_TRAIN, True, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-30-18:13:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./Checkpoints/checkpoints_tutorial17-1/\"/model.ckpt-375\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-30-18:13:53\n",
      "INFO:tensorflow:Saving dict for global step 375: accuracy = 0.96666664, average_loss = 0.053104945, global_step = 375, loss = 1.5931484\n",
      "Evaluation results\n",
      "   average_loss, was: 0.0531049445271\n",
      "   accuracy, was: 0.966666638851\n",
      "   global_step, was: 375\n",
      "   loss, was: 1.59314835072\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./Checkpoints/checkpoints_tutorial17-1/\"/model.ckpt-375\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "  I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "  I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "  I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Predictions:\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"  I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"  I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"  I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
